<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Bench-2-CoP systematically maps AI evaluation benchmarks to the EU AI Act's Code of Practice requirements to identify critical gaps in safety and compliance testing.">
  <meta name="keywords" content="EU AI Act, AI Safety, AI Benchmarks, Compliance, Gap Analysis, AI Regulation, DEXAI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bench-2-CoP: Aligning AI Evaluation with the EU AI Act Code of Practice</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.dexai.eu/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="">
            Code of Practice: Market Practices
          </a>
          <a class="navbar-item" href="">
            AEI - Agentic Eval Interface
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <figure class="image is-128x128" style="margin: 0 auto;">
            <img src="./static/images/dexai-logo.png" alt="DexAI Logo" />
          </figure>
          <h1 class="title is-1 publication-title">Bench-2-CoP: Aligning AI Evaluation with the EU AI Act Code of Practice</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Matteo Prandi</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Piercosma Bisconti</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Vincenzo Suriani</a><sup>1,2</sup>,
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block is-size-6"><sup>1</sup>Sapienza University of Rome,</span>
            <span class="author-block is-size-6"><sup>2</sup>DEXAI-Artificial Ethics</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/methodology_diagram.png" alt="Methodology Diagram" />
      <h2 class="subtitle has-text-centered">
        A systematic gap analysis mapping AI evaluation benchmarks to the EU AI Act's Code of Practice requirements to identify critical blind spots in safety and compliance testing.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Challenge</h2>
        <div class="content has-text-justified">
          <p>
            The rapid advancement of artificial intelligence (AI) demands robust evaluation frameworks, especially with emerging regulations like the <a href="https://artificialintelligenceact.eu/" target="_blank">EU AI Act</a> and its forthcoming <a href="https://www.europarl.europa.eu/news/en/press-room/20230609IPR96215/ai-act-meps-ready-to-negotiate-on-rules-for-safe-and-transparent-ai" target="_blank">Code of Practice</a> (CoP). AI developers rely on a suite of established benchmarks (e.g., MMLU, HellaSwag) to measure progress and safety. However, these benchmarks were often designed before comprehensive regulations existed, creating a potential and dangerous disconnect. Are we testing for the capabilities and risks that regulators and society are most concerned about?
          </p>
           <p>
            This study systematically analyzes the coverage of widely-used AI benchmarks against the EU AI Act's specified model capabilities, propensities, and affordances. Using a mixed-methods approach combining systematic literature review, multi-Large Language Model (LLM) content analysis, and expert validation, this research aims to identify and quantify these critical evaluation gaps. Our findings will highlight underrepresented high-risk categories in AI assessment, providing an empirical basis for aligning benchmark development with regulatory compliance and fostering safer AI.
          </p>
        </div>
      </div>
    </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Methodology Overview</h2>
        <div class="content has-text-justified">
          <p>
            Our study employs a rigorous, three-phase mixed-methods design to ensure a comprehensive and reliable assessment of benchmark coverage.
          </p>
            <ol>
              <li><b>Systematic Review and Corpus Creation:</b> We first conduct a systematic literature review to identify the most influential benchmarks used by major AI developers. This involves analyzing technical reports, model cards, and research publications to compile a representative corpus of benchmarks for analysis.</li>
              <li><b>Multi-LLM Content Analysis:</b> We use a multi-LLM approach, leveraging models like GPT-4, Claude 3, and Gemini, to perform a large-scale content analysis. Each task within the benchmark corpus is classified against the detailed evaluation categories derived from the AI Act Code of Practice. This allows for scalable and consistent initial mapping.</li>
              <li><b>Expert Validation and Gap Analysis:</b> The LLM-generated classifications are then rigorously validated by a panel of human experts in AI safety, policy, and evaluation. This hybrid intelligence approach ensures accuracy and nuanced interpretation. Finally, we conduct a statistical gap analysis to identify which regulatory categories are systematically under-tested.</li>
          </ol>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">Selected Benchmark Corpus</h3>
          <p class="content has-text-justified">
            A stratified sample of the industry's most widely-used benchmarks was selected, ensuring representation across general capabilities, safety, fairness, and reasoning. The table below details the selected benchmarks and the major AI labs that report using them for model evaluation.
          </p>
          <img src="./static/images/top_bench.png" alt="Anticipated Gap Analysis Chart showing benchmark coverage of different AI Act categories. Categories like 'Physical System Control', 'Self-Replication', and 'CBRN & Weapon Proliferation' show very low coverage, while 'Performance Reliability' and 'Hallucination' are better covered." />
          <!-- <div class="table-container">
            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                <thead>
                    <tr>
                        <th>Benchmark Name </th>
                        <th>Benchmark Type</th>
                        <th>OpenAI</th>
                        <th>Anthropic</th>
                        <th>Meta</th>
                        <th>Microsoft</th>
                        <th>Google</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td><a href="https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu" target="_blank">MMLU</a></td><td>General</td><td>X</td><td>X</td><td>X</td><td>X</td><td>X</td></tr>
                    <tr><td><a href="https://arxiv.org/abs/1905.07830" target="_blank">HellaSwag</a></td><td>General</td><td>X</td><td></td><td>X</td><td>X</td><td>X</td></tr>
                    <tr><td><a href="https://huggingface.co/datasets/allenai/ai2_arc" target="_blank">ARC Challenge</a></td><td>Reasoning</td><td>X</td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2210.09261" target="_blank">BIG-Bench Hard</a></td><td>General</td><td></td><td>X</td><td>X</td><td>X</td><td>X</td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2311.12022" target="_blank">GPQA</a></td><td>Reasoning</td><td></td><td>X</td><td></td><td>X</td><td>X</td></tr>
                    <tr><td><a href="https://huggingface.co/datasets/allenai/openbookqa" target="_blank">OpenBookQA</a></td><td>Understanding</td><td></td><td></td><td></td><td>X</td><td>X</td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2109.07958" target="_blank">TruthfulQA</a></td><td>Understanding</td><td>X</td><td></td><td></td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2110.08193" target="_blank">BBQ</a></td><td>Fairness and Bias</td><td>X</td><td>X</td><td></td><td></td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/pdf/1907.10641" target="_blank">WinoGrande</a></td><td>Understanding</td><td></td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2304.06364" target="_blank">AGIEval</a></td><td>General</td><td></td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/1811.00937" target="_blank">CommonSenseQA</a></td><td>Understanding</td><td></td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/1903.00161" target="_blank">DROP</a></td><td>Reasoning</td><td></td><td>X</td><td>X</td><td></td><td>X</td></tr>
                    <tr><td><a href="https://paperswithcode.com/dataset/medqa" target="_blank">MedQA</a></td><td>Health</td><td>X</td><td></td><td></td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/2406.01574" target="_blank">MMLU-Pro</a></td><td>General</td><td></td><td></td><td>X</td><td></td><td>X</td></tr>
                    <tr><td><a href="https://arxiv.org/abs/1911.11641" target="_blank">PiQA</a></td><td>Understanding</td><td></td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://arxiv.org/abs/1904.09728" target="_blank">SocialIQA</a></td><td>Understanding</td><td></td><td></td><td>X</td><td>X</td><td></td></tr>
                    <tr><td><a href="https://huggingface.co/datasets/cais/hle" target="_blank">Humanity Last Exam</a></td><td>General</td><td></td><td></td><td></td><td></td><td>X</td></tr>
                </tbody>
            </table>
          </div> -->
        </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h3 class="title is-4">The EU AI Act Evaluation Framework</h3>
          <div class="content has-text-justified">
              <p>
                  A detailed evaluation framework based on the EU AI Act and emerging Codes of Practice (CoP) was developed and refined with expert consultation. This framework categorizes regulatory concerns into three main areas, providing a structured lens through which to analyze benchmark content:
              </p>
              <ul>
                  <li><strong>Model Capabilities:</strong> This category assesses what the AI *can do*. It includes potentially high-risk functionalities such as Offensive Cyber Capabilities, advanced Deception, and autonomous Self-Replication. The goal is to determine if benchmarks are actively testing for the presence of these dangerous skills.</li>
                  <li><strong>Model Propensities:</strong> This focuses on the AI's inherent behavioral tendencies. Are benchmarks designed to detect Misalignment with human values, a propensity for Hallucination, or emergent power-seeking behaviors? This category moves beyond skill to evaluate inclination.</li>
                  <li><strong>Model Affordances & Contextual Factors:</strong> This area considers how a model's characteristics enable (mis)use depending on context. It includes factors like its Scalability for causing widespread harm, its Vulnerability to having safety guardrails removed, and its lack of Transparency. These are crucial for real-world risk assessment but are often overlooked in standard evaluations.</li>
              </ul>
          </div>
      </div>
    </div>

    <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-3">Visualizing the Gaps: Anticipated Findings</h2>
            <div class="content has-text-justified">
                <p>
                    While the full analysis is ongoing, our preliminary work and understanding of the landscape lead us to anticipate significant coverage gaps. We expect to find that most benchmarks focus heavily on performance for standard capabilities (e.g., language understanding, basic reasoning) while systematically under-testing for high-risk capabilities and propensities identified in the AI Act CoP. The chart below illustrates our anticipated findings, highlighting which areas are likely to be evaluation blind spots.
                </p>
                <img src="./static/images/overall_compliance.png" alt="Anticipated Gap Analysis Chart showing benchmark coverage of different AI Act categories. Categories like 'Physical System Control', 'Self-Replication', and 'CBRN & Weapon Proliferation' show very low coverage, while 'Performance Reliability' and 'Hallucination' are better covered." />
                <h4 class="title is-5 has-text-centered" style="margin-top: 1rem;">Potential Charting Strategies</h4>
                <p>In addition to the main gap analysis, this research will enable other key visualizations:</p>
                <ul>
                  <li><b>Coverage by Benchmark Type:</b> A grouped bar chart comparing the average coverage of "General" vs. "Safety/Fairness" vs. "Reasoning" benchmarks to see which categories are more safety-aware.</li>
                  <li><b>Developer-Specific Analysis:</b> Heatmaps showing the coverage patterns of benchmark suites preferred by different major AI developers, highlighting potential differences in evaluation priorities.</li>
                </ul>
              </div>
              <img src="./static/images/comm-level_compliance.png" alt="Anticipated Gap Analysis Chart showing benchmark coverage of different AI Act categories. Categories like 'Physical System Control', 'Self-Replication', and 'CBRN & Weapon Proliferation' show very low coverage, while 'Performance Reliability' and 'Hallucination' are better covered." />
        </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Implications for Stakeholders</h2>

        <div class="content has-text-justified">
          <p>The findings from this research will have direct and actionable implications for key groups in the AI ecosystem:</p>
            <ul>
                <li><b>For Policymakers & Regulators:</b> Our analysis will provide an empirical evidence base to inform the ongoing development of the AI Act's Codes of Practice and future standards for AI evaluation. It will highlight where regulatory guidance is most needed.</li>
                <li><b>For AI Developers:</b> The results will serve as a roadmap for augmenting internal evaluation suites to better align with compliance requirements. It will help developers identify blind spots in their testing pipelines before deploying high-risk systems.</li>
                <li><b>For the Research Community:</b> We provide a reusable methodology and a public dataset to foster further research into regulatory-aligned AI evaluation. It will encourage the creation of new benchmarks designed specifically to test for the risks identified in the AI Act.</li>
            </ul>
        </div>
      </div>
    </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{prandi2025bench2cop,
  author    = {Prandi, Matteo and Bisconti, Piercosma and Suriani, Vincenzo},
  title     = {Bench-2-CoP: Aligning AI Evaluation with the EU AI Act Code of Practice},
  booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/dexai-eu" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>